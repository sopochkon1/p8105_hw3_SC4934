---
title: "p8105_hw2_SC4934"
output: github_document
---
# library

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1


```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

# Dataset description

```{r, chunk_skim, results = "hide", message = FALSE}
skimr::skim(instacart)
```

In the instacart dataset, there are `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users

# how many aisles:

```{r}

instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

there are 134 aisles. The most orders are from the fresh vegetables aisle (`aisle_id` = 83), with 150609 orders. 

## plot

```{r}

instacart %>%
  group_by(aisle) %>%
  summarize(orders = n()) %>% 
  arrange(desc(orders)) %>%
  filter(orders > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, orders)) %>% 
  ggplot(aes(x = orders, y = aisle)) +
  geom_point() +
  labs(
    x = "N items ordered",
    y = "Aisle",
    title = "Number of items ordered in each aisle"
  ) + 
  scale_x_continuous(
    breaks = c(10000, 30000, 50000, 70000, 90000, 110000, 130000, 150000),
    labels = c("10000", "30000", "50000", "70000", "90000", "110000", "130000", "150000")
  ) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```


## table

this table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and the amount of times each item was ordered. 


```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

## table 2

this table shows 
 
```{r message=FALSE, warning=FALSE}

instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)

```
 
 
 this table shows the mean hour of day at whidh pink apples and coffee ice cream are ordered on each day of the week. the table was formatted in an untidy way so it is easier for humans to read. pink lady apples are mostly ordered earlier in the day than Coffee Ice Cream.
 
# Problem 2

## load and tidy

```{r warning = FALSE}
accel_data = 
  read_csv(
  "data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes",
    values_to = "activity_count",
    names_prefix = 'activity_'
  ) %>% 
  mutate(
    weekend_vs_weekday = case_when(day == "Saturday"| day == "Sunday" ~ "Weekend", 
    day == "Monday"| day == "Tuesday" | day == "Wednesday" | day =="Thursday" | day == "Friday" ~ "Weekday")
  )


```

the datasets includes `r nrow(accel_data)` rows and `r ncol(accel_data)` columns. the character variable `weekend_vs_weekday` was created. in total, there are `r accel_data %>% select(day_id) %>% distinct %>% count` days tracked in the dataset. the variables include week, day_id, minutes, activity_count, and weekend_vs_weekday.  


## Aggregate table

```{r warning = FALSE}
accel_data %>% 
  group_by(week, day) %>% 
  summarize(total = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total,
  ) %>% 
  select(week, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday) %>% 
  knitr::kable(digits=2) 

```

some of the trends show decreased activity during the weekend compared to the weekdays. For example, activity 4 and 5 are generally lower in the weekend than the weekdays. activity 1 starts off lower on Monday and increases steadily during the week. 



## plot 

```{r single panel plot, warning = FALSE}
accel_data %>% 
  mutate(
    minutes = as.integer(minutes)
  ) %>% 
  ggplot(aes(x = minutes/60, y = activity_count, color = day)) +
  geom_line() +
  geom_point(alpha = .5)+
  geom_smooth(se = FALSE) +
  labs(
    title = "24-hour activity per day",
    x = "Hour",
    y = "Activity"
  )

```

from this plot we can observe that in general, the highest level of activity was between 5am - 10pm, with activity dropping off after 10pm. The highest levels of avtivity are observed in the morning hours ~10am, and in the evening ~8-9pm, while there is a plateau in between those hours for most days. 

# Problem 3

## load data

```{r}
data("ny_noaa")

ny_noaa = 
  ny_noaa %>% 
  as_tibble(ny_noaa) %>% 
    janitor::clean_names() 


skimr::skim(ny_noaa)
```

the dataset `ny_noaa` contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. there are 3 character, 1 date, and 3 numeric variables. the variables include id, which is the weather station ID, date - date of observation, precipitation and snowfall information in `prcp`,`snow` and `snwd`, and maximum and minimum temperatures in `tmax` and `tmin`. there are `r ny_noaa %>% select(id) %>% distinct %>% count` weather stations tracked in the dataset. There is a significant amount of missing data, especially in `tmin` and `tmax`. 


## cleaning

```{r}
ny_noaa = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-", convert = TRUE) 
  
ny_noaa = ny_noaa %>% 
  mutate(
    prcp = as.integer(prcp),
    tmax = as.integer(tmax),
    tmin = as.integer(tmin),
    prcp = prcp/10,
    tmax = tmax/10,
    tmin = tmin/10,
    month = month.name[month]
  ) 

ny_noaa %>% 
  select(snow) %>% 
  table() %>% 
  sort(decreasing = TRUE)  %>%
  head(10) %>% view()



```

the most commonly observed value is 0 (frequency = 2008508), followed by 25 and 13 in the top 3. The most common ovserved value is 0 because it does not snow most of the time. 

## plot

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
ny_noaa %>% 
  filter(month %in% c("January", "July")) %>%
  group_by(id, month, year) %>% 
  summarise(mean_tmax = mean(tmax,na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = id, color = id)) +
  geom_line(alpha = .8) +
  theme(legend.position = "none") +
  facet_grid(.~month) +
  labs(title = "Average max temperature in January and July",
       x = "year",
       y = "average max temperatures"
       )


```

the graphs show that in January, majority of the stations detect temperatures that fluctuate between ~ -10 and 10 degrees Celsius, while in July they fluctuate between 20 to 35 degrees Celsius. There are outliers that detect temperatures as below -10 in January, as well as in July that detect temperatures lower than 20 such as ~14 degrees Celsius. 


## plot 2

```{r}
tmax_vs_tmin =
ny_noaa %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex()+
  labs(title = "tmax vs tmin",
       x = "minimum temperature",
       y = "maximum temperature"
       ) +
    theme(legend.position = "right") 
  

snowfall = 
  ny_noaa %>% 
  filter(snow %in% (1:99)) %>% 
  ggplot(aes(x = snow, y = as.factor(year), group = year)) +
  geom_density_ridges() +
  labs(
    title = "Snowfall Distribution Per Year",
    x = "snowfall in mm",
    y = "year"
  )

     

(snowfall / tmax_vs_tmin)
 


```

snowfall distribution per year shows that in most years, snowfall fluctuates and there are several peaks between 0 and 40, as well as one between 40 and 80 mm. there is generally higher frequency each year between 0-40mm than 40-80mm. 

second panel (tmin vs tmax): 















